# -*- coding: utf-8 -*-
"""Loan_Approval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vK4tWJn1Po-QXDIs1Y2V5JCCj6KIOa5c
"""

## Install the libraries
import pandas as pd
import numpy as np
import joblib
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('/content/Dataset.csv')
# df

# df.head()

# df.tail()

# df.shape

# df.columns

# df.info()

# df.isnull().sum()

# df.describe()

#BoxPlot

# plt.figure(figsize=(12,8))
# sns.boxplot(data = df)

# Fill missing values (Numerical)
df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].median())
df['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mean())
df['Credit_History'] = df['Credit_History'].fillna(df['Credit_History'].mean())

# Fill missing values (Categorical)
for col in ['Gender', 'Married', 'Dependents', 'Self_Employed']:
  df[col] = df[col].fillna(df[col].mode()[0])

# df.isnull().sum()

# print('Number of people who took loan by gender')
# print(df['Gender'].value_counts())
# sns.countplot(x='Gender',data = df, palette='Set2')

# print('Number of people who took loan by Married')
# print(df['Married'].value_counts())
# sns.countplot(x='Married',data = df, palette='Set3')

# print('Number of people who took loan by Education')
# print(df['Education'].value_counts())
# sns.countplot(x='Education',data = df, palette='Set1')

# Plot correlation heatmap in one step
# sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='BuPu', fmt='.2f', linewidths=0.5)
# plt.show()

# Feature Engineering
df['Total_Income'] = df['ApplicantIncome'] + df['CoapplicantIncome']
df['Total_Income_log'] = np.log(df['Total_Income'] + 1)

# df.head()

# Drop unnecessary columns (KEEPING LoanAmount)
df.drop(columns=['ApplicantIncome', 'CoapplicantIncome', 'Loan_Amount_Term', 'Total_Income', 'Loan_ID'], inplace=True)

# df.head()

# Encoding categorical features

from sklearn.preprocessing import LabelEncoder

#Encoding categorical features
def encode_categorical_columns(df):
    encoders = {}
    for col in ['Gender', 'Married', 'Education', 'Dependents', 'Self_Employed', 'Property_Area', 'Loan_Status']:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
        encoders[col] = le  # Store encoder for later use

    return df, encoders

# df.head()

# df.dtypes

# Split features and target variable
X = df.drop(columns=['Loan_Status'])
y = df['Loan_Status']

y

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25,random_state = 42)

# Logistic Regression
# model1 = LogisticRegression()
# model1.fit(X_train,y_train)
# y_pred_model1 = model1.predict(X_test)
# accuracy = accuracy_score(y_test,y_pred_model1)
# accuracy

# Decision Tree Classifier

# model2 = DecisionTreeClassifier()
# model2.fit(X_train,y_train)
# y_pred_model2 = model2.predict(X_test)
# accuracy = accuracy_score(y_test,y_pred_model2)
# print("Accuracy score of Decision Tree: ", accuracy*100)

# Random Forest Classifier
# model3 = RandomForestClassifier()
# model3.fit(X_train,y_train)
# y_pred_model3 = model3.predict(X_test)
# accuracy = accuracy_score(y_test,y_pred_model3)
# print("Accuracy score of Random Forest: ", accuracy*100)

#KNearestNeighbors model
# model4 = KNeighborsClassifier(n_neighbors=3)
# model4.fit(X_train,y_train)
# y_pred_model4 = model4.predict(X_test)
# accuracy = accuracy_score(y_test,y_pred_model4)
# print("Accuracy score of KNeighbors: ", accuracy*100)

# from sklearn.metrics import classification_report

# def generate_classification_report(model_name,y_test,y_pred):
#   report = classification_report(y_test,y_pred)
#   print(f"Classification Report For {model_name}:\n{report}\n")

# generate_classification_report(model1,y_test,y_pred_model1)
# generate_classification_report(model2,y_test,y_pred_model2)
# generate_classification_report(model3,y_test,y_pred_model3)
# generate_classification_report(model4,y_test,y_pred_model4)

# df['Loan_Status'].value_counts()

# pip install -U imbalanced-learn

from imblearn.over_sampling import RandomOverSampler

# Handle imbalanced data
oversample = RandomOverSampler(random_state=42)
X_resampled, y_resampled = oversample.fit_resample(X, y)

# X_resampled

# y_resampled

# y_resampled.value_counts()

# Train-test split
X_resampled_train, X_resampled_test, y_resampled_train, y_resampled_test = train_test_split(
    X_resampled, y_resampled, test_size=0.25, random_state=42
)

# Logistic Regression
# model1 = LogisticRegression()
# model1.fit(X_resampled_train,y_resampled_train)
# y_pred_model1 = model1.predict(X_resampled_test)
# accuracy = accuracy_score(y_resampled_test,y_pred_model1)
# accuracy*100

# Decision Tree Classifier

# model2 = DecisionTreeClassifier()
# model2.fit(X_resampled_train,y_resampled_train)
# y_pred_model2 = model2.predict(X_resampled_test)
# accuracy = accuracy_score(y_resampled_test,y_pred_model2)
# print("Accuracy score of Decision Tree: ", accuracy*100)

## Random Forest Classifier
model3 = RandomForestClassifier()
model3.fit(X_resampled_train,y_resampled_train)
y_pred_model3 = model3.predict(X_resampled_test)
accuracy = accuracy_score(y_resampled_test,y_pred_model3)
print("Accuracy score of Random Forest: ", accuracy*100)

#KNearestNeighbors model
# model4 = KNeighborsClassifier(n_neighbors=3)
# model4.fit(X_resampled_train,y_resampled_train)
# y_pred_model4 = model4.predict(X_resampled_test)
# accuracy = accuracy_score(y_resampled_test,y_pred_model4)
# print("Accuracy score of KNeighbors: ", accuracy*100)

# from sklearn.metrics import classification_report

# def generate_classification_report(model_name,y_test,y_pred):
#   report = classification_report(y_test,y_pred)
#   print(f"Classification Report For {model_name}:\n{report}\n")

# generate_classification_report(model1,y_resampled_test,y_pred_model1)
# generate_classification_report(model2,y_resampled_test,y_pred_model2)
# generate_classification_report(model3,y_resampled_test,y_pred_model3)
# generate_classification_report(model4,y_resampled_test,y_pred_model4)

# Train and save best model (model3)
def train_model():
    model3 = RandomForestClassifier()
    model3.fit(X_resampled_train, y_resampled_train)
    joblib.dump(model3, "loan_model.pkl")
    return model3

model3 = train_model()

# Evaluate Model
train_acc = accuracy_score(y_resampled_train, model3.predict(X_resampled_train))
test_acc = accuracy_score(y_resampled_test, model3.predict(X_resampled_test))